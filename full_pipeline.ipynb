{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tkw-X5DFf_N9"
      },
      "outputs": [],
      "source": [
        "from typing import Dict, Tuple\n",
        "from pathlib import Path\n",
        "import json\n",
        "import pickle\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "!pip install pandarallel\n",
        "from pandarallel import pandarallel\n",
        "pandarallel.initialize(progress_bar=True, verbose=0, nb_workers=10)\n",
        "\n",
        "from utils import load_json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aSpQPSf7gazl"
      },
      "outputs": [],
      "source": [
        "train_data = load_json(\"data/public_train.jsonl\")\n",
        "test_data = load_json(\"data/public_test_0316_user.jsonl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7MSEcenjVrL"
      },
      "source": [
        "# 1. Document retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "niqu9pLajYC_"
      },
      "outputs": [],
      "source": [
        "!pip install opencc\n",
        "!pip install hanlp\n",
        "!pip install wikipedia\n",
        "\n",
        "import opencc\n",
        "import wikipedia\n",
        "wikipedia.set_lang(\"zh\")\n",
        "\n",
        "import hanlp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3NU01DnjKp-"
      },
      "outputs": [],
      "source": [
        "# Helper functions\n",
        "\n",
        "converter = opencc.OpenCC(\"t2s.json\")\n",
        "st_converter = opencc.OpenCC(\"s2t.json\")\n",
        "\n",
        "def do_st_corrections(text):\n",
        "    simplified = converter.convert(text)\n",
        "    return st_converter.convert(simplified)\n",
        "\n",
        "\n",
        "def get_nps_hanlp(predictor, d) -> list:\n",
        "    # hanlp\n",
        "    claim = d[\"claim\"]\n",
        "    tree = predictor(claim)[\"con\"]\n",
        "    nps = [\n",
        "        do_st_corrections(\"\".join(subtree.leaves()))\n",
        "        for subtree in tree.subtrees(lambda t: t.label() == \"NP\")\n",
        "    ]\n",
        "    return nps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ayGI44qkk_wy"
      },
      "outputs": [],
      "source": [
        "# Main function for Document retrieval\n",
        "\n",
        "def get_pred_pages(x):\n",
        "    results = []\n",
        "    tmp_muji = []\n",
        "    mapping = {}  # wiki_page: its index showned in claim\n",
        "    claim = x[\"claim\"]\n",
        "    nps = x[\"hanlp_results\"]\n",
        "    first_wiki_term = []\n",
        "\n",
        "    for i, np in enumerate(nps):\n",
        "        wiki_search_results = [do_st_corrections(w) for w in wikipedia.search(np)]\n",
        "        wiki_set = [re.sub(r\"\\s\\(\\S+\\)\", \"\", w) for w in wiki_search_results]\n",
        "        wiki_df = pd.DataFrame(\n",
        "            {\"wiki_set\": wiki_set, \"wiki_results\": wiki_search_results}\n",
        "        )\n",
        "        grouped_df = wiki_df.groupby(\"wiki_set\", sort=False).first()\n",
        "        candidates = grouped_df[\"wiki_results\"].tolist()\n",
        "        muji = grouped_df.index.tolist()\n",
        "\n",
        "        for prefix, term in zip(muji, candidates):\n",
        "            if prefix not in tmp_muji:\n",
        "                matched = False\n",
        "                if i == 0:\n",
        "                    first_wiki_term.append(term)\n",
        "                if (\n",
        "                    ((new_term := term) in claim)\n",
        "                    or ((new_term := term.replace(\"·\", \"\")) in claim)\n",
        "                    or ((new_term := term.split(\" \")[0]) in claim)\n",
        "                    or ((new_term := term.replace(\"-\", \" \")) in claim) # TODO\n",
        "                ):\n",
        "                    matched = True\n",
        "\n",
        "                elif \"·\" in term:\n",
        "                    splited = term.split(\"·\")\n",
        "                    for split in splited:\n",
        "                        if (new_term := split) in claim:\n",
        "                            matched = True\n",
        "                            break\n",
        "\n",
        "                if matched:\n",
        "                    # post-processing\n",
        "                    term = term.replace(\" \", \"_\")\n",
        "                    term = term.replace(\"-\", \"\")\n",
        "                    results.append(term)\n",
        "                    mapping[term] = claim.find(new_term)\n",
        "                    tmp_muji.append(new_term)\n",
        "\n",
        "    if len(results) > 5:\n",
        "        assert -1 not in mapping.values()\n",
        "        results = sorted(mapping, key=mapping.get)[:5]\n",
        "    elif len(results) < 1:\n",
        "        results = first_wiki_term\n",
        "\n",
        "    return set(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-IFCakQxxkm"
      },
      "outputs": [],
      "source": [
        "# Load Hanlp predictor\n",
        "\n",
        "predictor = (\n",
        "    hanlp.pipeline()\n",
        "    .append(\n",
        "        hanlp.load(\"FINE_ELECTRA_SMALL_ZH\"),\n",
        "        output_key=\"tok\",\n",
        "    )\n",
        "    .append(\n",
        "        hanlp.load(\"CTB9_CON_ELECTRA_SMALL\"),\n",
        "        output_key=\"con\",\n",
        "        input_key=\"tok\",\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pviunJfyjfTo"
      },
      "outputs": [],
      "source": [
        "hanlp_file = f\"data/hanlp_con_results.pkl\"\n",
        "if Path(hanlp_file).exists():\n",
        "    with open(hanlp_file, \"rb\") as f:\n",
        "        hanlp_results = pickle.load(f)\n",
        "\n",
        "else:\n",
        "    hanlp_results = [get_nps_hanlp(predictor, d) for d in train_data]\n",
        "    # save hanlp results\n",
        "    with open(hanlp_file, \"wb\") as f:\n",
        "        pickle.dump(hanlp_results, f)\n",
        "\n",
        "train_df = pd.DataFrame(train_data)\n",
        "train_df.loc[:, \"hanlp_results\"] = hanlp_results\n",
        "predicted_results = train_df.parallel_apply(get_pred_pages, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PjIoJoZwBF0R"
      },
      "outputs": [],
      "source": [
        "predicted_results.shape\n",
        "# print(len(train_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IT-dnZ2SmX3j"
      },
      "outputs": [],
      "source": [
        "# Helper functions\n",
        "\n",
        "def calculate_precision(data: list, predictions: pd.Series):\n",
        "    precision = 0\n",
        "    precision_hits = 0\n",
        "\n",
        "    for i, d in enumerate(data):\n",
        "        if d[\"label\"] == \"NOT ENOUGH INFO\":\n",
        "            continue\n",
        "        gt_pages = set(\n",
        "            [evidence[2] for evidence_set in d[\"evidence\"] for evidence in evidence_set]\n",
        "        )\n",
        "        predicted_pages = predictions.iloc[i]\n",
        "        tmp_precision = predicted_pages.intersection(gt_pages)\n",
        "        if len(predicted_pages) != 0:\n",
        "            precision += len(tmp_precision) / len(predicted_pages)\n",
        "        precision_hits += 1\n",
        "    print(f\"Precision: {precision / precision_hits}\")\n",
        "\n",
        "\n",
        "def calculate_recall(data: list, predictions: pd.Series):\n",
        "    recall = 0\n",
        "    recall_hits = 0\n",
        "\n",
        "    for i, d in enumerate(data):\n",
        "        if d[\"label\"] == \"NOT ENOUGH INFO\":\n",
        "            continue\n",
        "        gt_pages = set(\n",
        "            [evidence[2] for evidence_set in d[\"evidence\"] for evidence in evidence_set]\n",
        "        )\n",
        "        predicted_pages = predictions.iloc[i]\n",
        "        tmp_recall = predicted_pages.intersection(gt_pages)\n",
        "        recall += len(tmp_recall) / len(gt_pages)\n",
        "        recall_hits += 1\n",
        "    print(f\"Recall: {recall / recall_hits}\")\n",
        "\n",
        "\n",
        "def save_aicup_data(\n",
        "    data: list,\n",
        "    predictions: pd.Series,\n",
        "    mode: str = \"train\",\n",
        "    num_pred_doc: int = 5,\n",
        "):\n",
        "    with open(f\"data/{mode}_doc{num_pred_doc}.jsonl\", \"w\") as f:\n",
        "        for i, d in enumerate(data):\n",
        "            d[\"predicted_pages\"] = list(predictions.iloc[i])\n",
        "            f.write(json.dumps(d, ensure_ascii=False) + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7B3lnp8mcYo"
      },
      "outputs": [],
      "source": [
        "calculate_precision(train_data, predicted_results)\n",
        "calculate_recall(train_data, predicted_results)\n",
        "save_aicup_data(train_data, predicted_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8TatjXdLnDPS"
      },
      "outputs": [],
      "source": [
        "# Predict for the test data\n",
        "\n",
        "# Get NPs using Hanlp\n",
        "hanlp_results = [get_nps_hanlp(predictor, d) for d in test_data]\n",
        "\n",
        "# Transform the test data into pd.DataFrame\n",
        "test_df = pd.DataFrame(test_data)\n",
        "test_df.loc[:, \"hanlp_results\"] = hanlp_results\n",
        "\n",
        "# Do document retrieval for the test data\n",
        "predicted_results = test_df.parallel_apply(get_pred_pages, axis=1)\n",
        "save_aicup_data(test_data, predicted_results, mode=\"test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ol4zFkSbjgXF"
      },
      "source": [
        "# 2. Sentence retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GlliDsgXjisj"
      },
      "outputs": [],
      "source": [
        "# !pip install transformers\n",
        "# !pip install pandarallel\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pathlib import Path\n",
        "from tqdm.auto import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from pandarallel import pandarallel\n",
        "pandarallel.initialize(progress_bar=True, verbose=0, nb_workers=10)\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.optim import AdamW\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    get_scheduler,\n",
        ")\n",
        "\n",
        "from utils import (\n",
        "    load_json,\n",
        "    jsonl_dir_to_df,\n",
        "    generate_evidence_to_wiki_pages_mapping,\n",
        "    set_lr_scheduler,\n",
        "    save_checkpoint,\n",
        "    load_model,\n",
        ")\n",
        "from dataset import BERTDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J3BBLE3_hlPi"
      },
      "outputs": [],
      "source": [
        "train_data = load_json(\"data/train_doc5.jsonl\")\n",
        "label_map = {\"supports\": 0, \"refutes\": 1, \"NOT ENOUGH INFO\": 2}\n",
        "\n",
        "SEED = 42\n",
        "\n",
        "_y = [label_map[d[\"label\"]] for d in train_data]\n",
        "train_gt, dev_gt = train_test_split(\n",
        "    train_data, test_size=0.2, random_state=SEED, shuffle=True, stratify=_y\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MHLPu8vlfm1F"
      },
      "outputs": [],
      "source": [
        "# 下載 `wiki-pages.zip`\n",
        "# 此檔案與 Tbrain 網站上的資料是一樣的\n",
        "\n",
        "# !wget 140.116.245.105:33332/wiki-pages.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uG4xUNwasJUd"
      },
      "outputs": [],
      "source": [
        "# 將 wiki-pages.zip 解壓縮至 data/ 底下\n",
        "# 指令：unzip `欲解壓縮檔案之位置` -d `解壓縮後的目的地`\n",
        "\n",
        "# !unzip \"wiki-pages.zip\" -d \"data/\"\n",
        "wiki_pages = jsonl_dir_to_df(\"data/wiki-pages\")\n",
        "mapping = generate_evidence_to_wiki_pages_mapping(\n",
        "    wiki_pages,\n",
        ")\n",
        "del wiki_pages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpvXpFwXszfv"
      },
      "outputs": [],
      "source": [
        "def pair_with_wiki_sentences(\n",
        "    mapping: dict,\n",
        "    df: pd.DataFrame,\n",
        "    negative_ratio: float,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"Only for creating train sentences.\"\"\"\n",
        "    claims = []\n",
        "    sentences = []\n",
        "    labels = []\n",
        "\n",
        "    # positive\n",
        "    for i in range(len(df)):\n",
        "        if df[\"label\"].iloc[i] == \"NOT ENOUGH INFO\":\n",
        "            continue\n",
        "        claim = df[\"claim\"].iloc[i]\n",
        "        evidence_sets = df[\"evidence\"].iloc[i]\n",
        "        for evidence_set in evidence_sets:\n",
        "            sents = []\n",
        "            for evidence in evidence_set:\n",
        "                page = evidence[2].replace(\" \", \"_\")\n",
        "                if page == \"臺灣海峽危機#第二次臺灣海峽危機（1958）\":\n",
        "                    continue\n",
        "                sent_idx = str(evidence[3])\n",
        "                sents.append(mapping[page][sent_idx])\n",
        "\n",
        "            whole_evidence = \" \".join(sents)\n",
        "\n",
        "            claims.append(claim)\n",
        "            sentences.append(whole_evidence)\n",
        "            labels.append(1)\n",
        "\n",
        "    # negative\n",
        "    for i in range(len(df)):\n",
        "        if df[\"label\"].iloc[i] == \"NOT ENOUGH INFO\":\n",
        "            continue\n",
        "        claim = df[\"claim\"].iloc[i]\n",
        "\n",
        "        evidence_set = set([(evidence[2], evidence[3])\n",
        "                            for evidences in df[\"evidence\"][i]\n",
        "                            for evidence in evidences])\n",
        "        predicted_pages = df[\"predicted_pages\"][i]\n",
        "        for page in predicted_pages:\n",
        "            page = page.replace(\" \", \"_\")\n",
        "            # ('城市規劃', sent_idx)\n",
        "            try:\n",
        "                page_sent_id_pairs = [(page, sent_idx) for sent_idx in mapping[page].keys()]\n",
        "            except KeyError:\n",
        "                # print(f\"{page} is not in our Wiki db.\")\n",
        "                continue\n",
        "\n",
        "            for pair in page_sent_id_pairs:\n",
        "                if pair in evidence_set:\n",
        "                    continue\n",
        "                text = mapping[page][pair[1]]\n",
        "                # `np.random.rand(1) <= 0.05`: Control not to add too many negative samples\n",
        "                if text != \"\" and np.random.rand(1) <= negative_ratio:\n",
        "                    claims.append(claim)\n",
        "                    sentences.append(text)\n",
        "                    labels.append(0)\n",
        "\n",
        "    return pd.DataFrame({\"claim\": claims, \"text\": sentences, \"label\": labels})\n",
        "\n",
        "\n",
        "def pair_with_wiki_sentences_eval(\n",
        "    mapping: dict,\n",
        "    df: pd.DataFrame,\n",
        "    is_testset: bool = False,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"Only for creating dev and test sentences.\"\"\"\n",
        "    claims = []\n",
        "    sentences = []\n",
        "    evidence = []\n",
        "    predicted_evidence = []\n",
        "\n",
        "    # negative\n",
        "    for i in range(len(df)):\n",
        "        # if df[\"label\"].iloc[i] == \"NOT ENOUGH INFO\":\n",
        "        #     continue\n",
        "        claim = df[\"claim\"].iloc[i]\n",
        "\n",
        "        predicted_pages = df[\"predicted_pages\"][i]\n",
        "        for page in predicted_pages:\n",
        "            page = page.replace(\" \", \"_\")\n",
        "            # ('城市規劃', sent_idx)\n",
        "            try:\n",
        "                page_sent_id_pairs = [(page, k) for k in mapping[page]]\n",
        "            except KeyError:\n",
        "                # print(f\"{page} is not in our Wiki db.\")\n",
        "                continue\n",
        "\n",
        "            for pair in page_sent_id_pairs:\n",
        "                text = mapping[page][pair[1]]\n",
        "                if text != \"\":\n",
        "                    claims.append(claim)\n",
        "                    sentences.append(text)\n",
        "                    if not is_testset:\n",
        "                        evidence.append(df[\"evidence\"].iloc[i])\n",
        "                    predicted_evidence.append([pair[0], int(pair[1])])\n",
        "\n",
        "    return pd.DataFrame({\n",
        "        \"claim\": claims,\n",
        "        \"text\": sentences,\n",
        "        \"evidence\": evidence if not is_testset else None,\n",
        "        \"predicted_evidence\": predicted_evidence,\n",
        "    })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ToLvE9oxIXQo"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "\n",
        "MODEL_NAME = \"bert-base-chinese\"\n",
        "NUM_EPOCHS = 1\n",
        "LR = 2e-5\n",
        "TRAIN_BATCH_SIZE = 64\n",
        "TEST_BATCH_SIZE = 256\n",
        "NEGATIVE_RATIO = 0.03\n",
        "VALIDATION_STEP = 50\n",
        "TOP_N = 5\n",
        "\n",
        "exp_dir = (\n",
        "    f\"sent_retrieval/0312_e{NUM_EPOCHS}_bs{TRAIN_BATCH_SIZE}_\"\n",
        "    + f\"{LR}_neg{NEGATIVE_RATIO}_top{TOP_N}\"\n",
        ")\n",
        "log_dir = \"logs/\" + exp_dir\n",
        "ckpt_dir = \"checkpoints/\" + exp_dir\n",
        "\n",
        "if not Path(log_dir).exists():\n",
        "    Path(log_dir).mkdir(parents=True)\n",
        "\n",
        "if not Path(ckpt_dir).exists():\n",
        "    Path(ckpt_dir).mkdir(parents=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4A5vWEzPiXGF"
      },
      "outputs": [],
      "source": [
        "train_df = pair_with_wiki_sentences(mapping, pd.DataFrame(train_gt), NEGATIVE_RATIO)\n",
        "counts = train_df[\"label\"].value_counts()\n",
        "print(\"Now using the following train data with 0 (Negative) and 1 (Positive)\")\n",
        "print(counts)\n",
        "\n",
        "dev_evidences = pair_with_wiki_sentences_eval(mapping, pd.DataFrame(dev_gt))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PnsSDqcvidWK"
      },
      "outputs": [],
      "source": [
        "# Helper functions for evaluation\n",
        "\n",
        "def evidence_macro_precision(\n",
        "    instance: dict, top_rows: pd.DataFrame\n",
        ") -> Tuple[float, float]:\n",
        "    \"\"\"Calculate precision for sentence retrieval\n",
        "    This function is modified from fever-scorer.\n",
        "    https://github.com/sheffieldnlp/fever-scorer/blob/master/src/fever/scorer.py\n",
        "\n",
        "    Args:\n",
        "        instance (dict): a row of the dev set (dev.jsonl) of test set (test.jsonl)\n",
        "        top_rows (pd.DataFrame): our predictions with the top probabilities\n",
        "\n",
        "        IMPORTANT!!!\n",
        "        instance (dict) should have the key of `evidence`.\n",
        "        top_rows (pd.DataFrame) should have a column `predicted_evidence`.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[float, float]:\n",
        "        [1]: relevant and retrieved (numerator of precision)\n",
        "        [2]: retrieved (denominator of precision)\n",
        "    \"\"\"\n",
        "    this_precision = 0.0\n",
        "    this_precision_hits = 0.0\n",
        "\n",
        "    if instance[\"label\"].upper() != \"NOT ENOUGH INFO\":\n",
        "        all_evi = [\n",
        "            [e[2], e[3]] for eg in instance[\"evidence\"] for e in eg if e[3] is not None\n",
        "        ]\n",
        "        claim = instance[\"claim\"]\n",
        "        predicted_evidence = top_rows[top_rows[\"claim\"] == claim][\n",
        "            \"predicted_evidence\"\n",
        "        ].tolist()\n",
        "\n",
        "        for prediction in predicted_evidence:\n",
        "            if prediction in all_evi:\n",
        "                this_precision += 1.0\n",
        "            this_precision_hits += 1.0\n",
        "\n",
        "        return (\n",
        "            this_precision / this_precision_hits\n",
        "        ) if this_precision_hits > 0 else 1.0, 1.0\n",
        "\n",
        "    return 0.0, 0.0\n",
        "\n",
        "\n",
        "def evidence_macro_recall(\n",
        "    instance: dict, top_rows: pd.DataFrame\n",
        ") -> Tuple[float, float]:\n",
        "    \"\"\"Calculate recall for sentence retrieval\n",
        "    This function is modified from fever-scorer.\n",
        "    https://github.com/sheffieldnlp/fever-scorer/blob/master/src/fever/scorer.py\n",
        "\n",
        "    Args:\n",
        "        instance (dict): a row of the dev set (dev.jsonl) of test set (test.jsonl)\n",
        "        top_rows (pd.DataFrame): our predictions with the top probabilities\n",
        "\n",
        "        IMPORTANT!!!\n",
        "        instance (dict) should have the key of `evidence`.\n",
        "        top_rows (pd.DataFrame) should have a column `predicted_evidence`.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[float, float]:\n",
        "        [1]: relevant and retrieved (numerator of recall)\n",
        "        [2]: relevant (denominator of recall)\n",
        "    \"\"\"\n",
        "    # We only want to score F1/Precision/Recall of recalled evidence for NEI claims\n",
        "    if instance[\"label\"].upper() != \"NOT ENOUGH INFO\":\n",
        "        # If there's no evidence to predict, return 1\n",
        "        if len(instance[\"evidence\"]) == 0 or all([len(eg) == 0 for eg in instance]):\n",
        "            return 1.0, 1.0\n",
        "\n",
        "        claim = instance[\"claim\"]\n",
        "\n",
        "        predicted_evidence = top_rows[top_rows[\"claim\"] == claim][\n",
        "            \"predicted_evidence\"\n",
        "        ].tolist()\n",
        "\n",
        "        for evidence_group in instance[\"evidence\"]:\n",
        "            evidence = [[e[2], e[3]] for e in evidence_group]\n",
        "            if all([item in predicted_evidence for item in evidence]):\n",
        "                # We only want to score complete groups of evidence. Incomplete groups are worthless.\n",
        "                return 1.0, 1.0\n",
        "        return 0.0, 1.0\n",
        "    return 0.0, 0.0\n",
        "\n",
        "\n",
        "def evaluate_retrieval(\n",
        "    probs: np.ndarray,\n",
        "    df_evidences: pd.DataFrame,\n",
        "    ground_truths: pd.DataFrame,\n",
        "    top_n: int = 5,\n",
        "    cal_scores: bool = True,\n",
        "    save_name: str = None,\n",
        ") -> dict[float, float, float]:\n",
        "    \"\"\"Calculate the scores of sentence retrieval\n",
        "\n",
        "    Args:\n",
        "        probs (np.ndarray): probabilities of the candidate retrieved sentences\n",
        "        df_evidences (pd.DataFrame): the candiate evidence sentences paired with claims\n",
        "        ground_truths (pd.DataFrame): the loaded data of dev.jsonl or test.jsonl\n",
        "        top_n (int, optional): the number of the retrieved sentences. Defaults to 2.\n",
        "\n",
        "    Returns:\n",
        "        dict[float, float, float]: F1 score, precision, and recall\n",
        "    \"\"\"\n",
        "    df_evidences[\"prob\"] = probs\n",
        "    top_rows = (\n",
        "        df_evidences.groupby(\"claim\")\n",
        "        .apply(lambda x: x.nlargest(top_n, \"prob\"))\n",
        "        .reset_index(drop=True)\n",
        "    )\n",
        "    if cal_scores:\n",
        "        macro_precision = 0\n",
        "        macro_precision_hits = 0\n",
        "        macro_recall = 0\n",
        "        macro_recall_hits = 0\n",
        "\n",
        "        for i, instance in enumerate(ground_truths):\n",
        "            macro_prec = evidence_macro_precision(instance, top_rows)\n",
        "            macro_precision += macro_prec[0]\n",
        "            macro_precision_hits += macro_prec[1]\n",
        "\n",
        "            macro_rec = evidence_macro_recall(instance, top_rows)\n",
        "            macro_recall += macro_rec[0]\n",
        "            macro_recall_hits += macro_rec[1]\n",
        "\n",
        "        pr = (macro_precision / macro_precision_hits) if macro_precision_hits > 0 else 1.0\n",
        "        rec = (macro_recall / macro_recall_hits) if macro_recall_hits > 0 else 0.0\n",
        "        f1 = 2.0 * pr * rec / (pr + rec)\n",
        "\n",
        "    if save_name is not None:\n",
        "        # write doc7_sent5 file\n",
        "        with open(f\"data/{save_name}\", \"w\") as f:\n",
        "            for instance in ground_truths:\n",
        "                claim = instance[\"claim\"]\n",
        "                predicted_evidence = top_rows[top_rows[\"claim\"] == claim][\n",
        "                    \"predicted_evidence\"\n",
        "                ].tolist()\n",
        "                instance[\"predicted_evidence\"] = predicted_evidence\n",
        "                f.write(json.dumps(instance, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "    if cal_scores:\n",
        "        return {\"F1 score\": f1, \"Precision\": pr, \"Recall\": rec}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5e_9wje6yiA7"
      },
      "outputs": [],
      "source": [
        "def get_predicted_probs(model, dataloader) -> np.ndarray:\n",
        "    \"\"\"Inference script to get probabilites for the candidate evidence sentences\n",
        "\n",
        "    Args:\n",
        "        model: the one from HuggingFace Transformers\n",
        "        dataloader: devset or testset in torch dataloader\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: probabilites of the candidate evidence sentences\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    probs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader):\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            outputs = model(**batch)\n",
        "            logits = outputs.logits\n",
        "            probs.extend(torch.softmax(logits, dim=1)[:, 1].tolist())\n",
        "\n",
        "    return np.array(probs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0JuNMG0OJsvz"
      },
      "outputs": [],
      "source": [
        "class SentRetrievalBERTDataset(BERTDataset):\n",
        "    \"\"\"AicupTopkEvidenceBERTDataset class for AICUP dataset with top-k evidence sentences.\"\"\"\n",
        "\n",
        "    def __getitem__(\n",
        "        self,\n",
        "        idx: int,\n",
        "        **kwargs,\n",
        "    ) -> Tuple[Dict[str, torch.Tensor], int]:\n",
        "        item = self.data.iloc[idx]\n",
        "        sentA = item[\"claim\"]\n",
        "        sentB = item[\"text\"]\n",
        "\n",
        "        # concat_claim_evidence = \" [SEP] \".join([sentA, sentB])\n",
        "\n",
        "        concat = self.tokenizer(\n",
        "            sentA,\n",
        "            sentB,\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.max_length,\n",
        "            truncation=True,\n",
        "        )\n",
        "        concat_ten = {k: torch.tensor(v) for k, v in concat.items()}\n",
        "        if \"label\" in item:\n",
        "            concat_ten[\"labels\"] = torch.tensor(item[\"label\"])\n",
        "        return concat_ten"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l48WifjeIGui"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = \"bert-base-chinese\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Set up torch Datasets.\n",
        "train_dataset = SentRetrievalBERTDataset(train_df, tokenizer=tokenizer)\n",
        "val_dataset = SentRetrievalBERTDataset(dev_evidences, tokenizer=tokenizer)\n",
        "\n",
        "# Set up torch DataLoaders.\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset, shuffle=True, batch_size=TRAIN_BATCH_SIZE\n",
        ")\n",
        "eval_dataloader = DataLoader(val_dataset, batch_size=TEST_BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4rl_u0YbeQtY"
      },
      "outputs": [],
      "source": [
        "# Set up our model.\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
        "model.to(device)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=LR)\n",
        "num_training_steps = NUM_EPOCHS * len(train_dataloader)\n",
        "lr_scheduler = set_lr_scheduler(optimizer, num_training_steps)\n",
        "\n",
        "# Set up the TensorBoard writer\n",
        "writer = SummaryWriter(log_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1AHGaKh1eKmg"
      },
      "outputs": [],
      "source": [
        "# Get ready to training.\n",
        "progress_bar = tqdm(range(num_training_steps))\n",
        "current_steps = 0\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    model.train()\n",
        "\n",
        "    for batch in train_dataloader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        progress_bar.update(1)\n",
        "        writer.add_scalar(\"training_loss\", loss.item(), current_steps)\n",
        "\n",
        "        y_pred = torch.argmax(outputs.logits, dim=1).tolist()\n",
        "\n",
        "        y_true = batch[\"labels\"].tolist()\n",
        "        # print(f\"batch train acc: {accuracy_score(y_true, y_pred)}\")\n",
        "\n",
        "        current_steps += 1\n",
        "\n",
        "        if current_steps % VALIDATION_STEP == 0 and current_steps > 0:\n",
        "            print(\"Start validation\")\n",
        "            probs = get_predicted_probs(model, eval_dataloader)\n",
        "\n",
        "            val_results = evaluate_retrieval(\n",
        "                probs=probs,\n",
        "                df_evidences=dev_evidences,\n",
        "                ground_truths=dev_gt,\n",
        "                top_n=TOP_N,\n",
        "            )\n",
        "            print(val_results)\n",
        "            # log each metric separately to TensorBoard\n",
        "            for metric_name, metric_value in val_results.items():\n",
        "                writer.add_scalar(f\"dev_{metric_name}\", metric_value, current_steps)\n",
        "\n",
        "            save_checkpoint(model, ckpt_dir, current_steps)\n",
        "\n",
        "print(\"Finished training!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QS1Ei5DAIO5p"
      },
      "outputs": [],
      "source": [
        "model = load_model(model, ckpt_dir, step=50)\n",
        "print(\"Start final evaluations and write prediction files.\")\n",
        "\n",
        "del train_df\n",
        "train_evidences = pair_with_wiki_sentences_eval(wiki_pages, pd.DataFrame(train_gt))\n",
        "train_set = SentRetrievalBERTDataset(train_evidences, tokenizer)\n",
        "train_dataloader = DataLoader(train_set, batch_size=TEST_BATCH_SIZE)\n",
        "\n",
        "print(\"Start calculating training scores\")\n",
        "probs = get_predicted_probs(model, train_dataloader)\n",
        "train_results = evaluate_retrieval(\n",
        "    probs=probs,\n",
        "    df_evidences=train_evidences,\n",
        "    ground_truths=train_gt,\n",
        "    top_n=TOP_N,\n",
        "    save_name=f\"train_doc5sent{TOP_N}.jsonl\",\n",
        ")\n",
        "print(f\"Training scores => {train_results}\")\n",
        "\n",
        "print(\"Start validation\")\n",
        "probs = get_predicted_probs(model, eval_dataloader)\n",
        "val_results = evaluate_retrieval(\n",
        "    probs=probs,\n",
        "    df_evidences=dev_evidences,\n",
        "    ground_truths=dev_gt,\n",
        "    top_n=TOP_N,\n",
        "    save_name=f\"dev_doc5sent{TOP_N}.jsonl\",\n",
        ")\n",
        "print(f\"Validation scores => {val_results}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lVFusJqjmex-"
      },
      "outputs": [],
      "source": [
        "test_data = load_json(\"data/test_doc5.jsonl\")\n",
        "\n",
        "test_evidences = pair_with_wiki_sentences_eval(wiki_pages, pd.DataFrame(test_data), is_testset=True)\n",
        "test_set = SentRetrievalBERTDataset(test_evidences, tokenizer)\n",
        "test_dataloader = DataLoader(test_set, batch_size=TEST_BATCH_SIZE)\n",
        "\n",
        "print(\"Start predicting the test data\")\n",
        "probs = get_predicted_probs(model, test_dataloader)\n",
        "evaluate_retrieval(\n",
        "    probs=probs,\n",
        "    df_evidences=test_evidences,\n",
        "    ground_truths=test_data,\n",
        "    top_n=TOP_N,\n",
        "    cal_scores=False,\n",
        "    save_name=f\"test_doc5sent{TOP_N}.jsonl\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGzl8a5JteT7"
      },
      "source": [
        "# 3. Claim verification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tgA1vcUyzjlx"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install pandarallel\n",
        "\n",
        "from typing import Dict, Tuple\n",
        "from pathlib import Path\n",
        "from tqdm.auto import tqdm\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandarallel import pandarallel\n",
        "\n",
        "pandarallel.initialize(progress_bar=True, verbose=0, nb_workers=4)\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.optim import AdamW\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    get_scheduler,\n",
        ")\n",
        "from sklearn.metrics import accuracy_score\n",
        "from utils import (\n",
        "    load_json,\n",
        "    jsonl_dir_to_df,\n",
        "    generate_evidence_to_wiki_pages_mapping,\n",
        "    set_lr_scheduler,\n",
        "    save_checkpoint,\n",
        "    load_model,\n",
        ")\n",
        "from dataset import BERTDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xSidldltu6cf"
      },
      "outputs": [],
      "source": [
        "label2ID: Dict[str, int] = {\n",
        "    \"supports\": 0,\n",
        "    \"refutes\": 1,\n",
        "    \"NOT ENOUGH INFO\": 2,\n",
        "}\n",
        "ID2label: Dict[int, str] = {v: k for k, v in label2ID.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e16r6JvZvzNs"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "\n",
        "TRAIN_BATCH_SIZE = 64\n",
        "TEST_BATCH_SIZE = 32\n",
        "LR = 7e-5\n",
        "NUM_EPOCHS = 20\n",
        "MAX_SEQ_LEN = 256\n",
        "EVIDENCE_TOPK = 5\n",
        "VALIDATION_STEP = 25\n",
        "\n",
        "OUTPUT_FILENAME = \"submission.jsonl\"\n",
        "\n",
        "exp_dir = (\n",
        "    f\"claim_verification/e{NUM_EPOCHS}_bs{TRAIN_BATCH_SIZE}_\"\n",
        "    + f\"{LR}_top{EVIDENCE_TOPK}\"\n",
        ")\n",
        "log_dir = \"logs/\" + exp_dir\n",
        "ckpt_dir = \"checkpoints/\" + exp_dir\n",
        "\n",
        "if not Path(log_dir).exists():\n",
        "    Path(log_dir).mkdir(parents=True)\n",
        "\n",
        "if not Path(ckpt_dir).exists():\n",
        "    Path(ckpt_dir).mkdir(parents=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6nWii59y0cK"
      },
      "outputs": [],
      "source": [
        "def join_with_topk_evidence(\n",
        "    df: pd.DataFrame,\n",
        "    mapping: dict,\n",
        "    mode: str = \"train\",\n",
        "    topk: int = 5,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"join_with_topk_evidence join the dataset with topk evidence.\n",
        "\n",
        "    Note:\n",
        "        After extraction, the dataset will be like this:\n",
        "               id     label         claim                           evidence            evidence_list\n",
        "        0    4604  supports       高行健...     [[[3393, 3552, 高行健, 0], [...  [高行健 （ ）江西赣州出...\n",
        "        ..    ...       ...            ...                                ...                     ...\n",
        "        945  2095  supports       美國總...  [[[1879, 2032, 吉米·卡特, 16], [...  [卸任后 ， 卡特積極參與...\n",
        "        停各种战争及人質危機的斡旋工作 ， 反对美国小布什政府攻打伊拉克...\n",
        "\n",
        "        [946 rows x 5 columns]\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The dataset with evidence.\n",
        "        wiki_pages (pd.DataFrame): The wiki pages dataframe\n",
        "        topk (int, optional): The topk evidence. Defaults to 5.\n",
        "        cache(Union[Path, str], optional): The cache file path. Defaults to None.\n",
        "            If cache is None, return the result directly.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The dataset with topk evidence_list.\n",
        "            The `evidence_list` column will be: List[str]\n",
        "    \"\"\"\n",
        "\n",
        "    # format evidence column to List[List[Tuple[str, str, str, str]]]\n",
        "    if \"evidence\" in df.columns:\n",
        "        df[\"evidence\"] = df[\"evidence\"].parallel_map(\n",
        "            lambda x: [[x]]\n",
        "            if not isinstance(x[0], list)\n",
        "            else [x]\n",
        "            if not isinstance(x[0][0], list)\n",
        "            else x\n",
        "        )\n",
        "\n",
        "    print(f\"Extracting evidence_list for the {mode} mode ...\")\n",
        "    if mode == \"eval\":\n",
        "        # extract evidence\n",
        "        df[\"evidence_list\"] = df[\"predicted_evidence\"].parallel_map(\n",
        "            lambda x: [\n",
        "                mapping.get(evi_id, {}).get(str(evi_idx), \"\")\n",
        "                for evi_id, evi_idx in x  # for each evidence list\n",
        "            ][:topk]\n",
        "            if isinstance(x, list)\n",
        "            else []\n",
        "        )\n",
        "        print(df[\"evidence_list\"][:5])\n",
        "    else:\n",
        "        # extract evidence\n",
        "        df[\"evidence_list\"] = df[\"evidence\"].parallel_map(\n",
        "            lambda x: [\n",
        "                \" \".join(\n",
        "                    [  # join evidence\n",
        "                        mapping.get(evi_id, {}).get(str(evi_idx), \"\")\n",
        "                        for _, _, evi_id, evi_idx in evi_list\n",
        "                    ]\n",
        "                )\n",
        "                if isinstance(evi_list, list)\n",
        "                else \"\"\n",
        "                for evi_list in x  # for each evidence list\n",
        "            ][:1]\n",
        "            if isinstance(x, list)\n",
        "            else []\n",
        "        )\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76Yd4mFrF4f3"
      },
      "outputs": [],
      "source": [
        "# 下載 `wiki-pages.zip`\n",
        "# 此檔案與 Tbrain 網站上的資料是一樣的\n",
        "\n",
        "!wget 140.116.245.105:33332/wiki-pages.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DZEBpH5q01ej"
      },
      "outputs": [],
      "source": [
        "# 將 wiki-pages.zip 解壓縮至 data/ 底下\n",
        "# 指令：unzip `欲解壓縮檔案之位置` -d `解壓縮後的目的地`\n",
        "\n",
        "!unzip \"wiki-pages.zip\" -d \"data/\"\n",
        "wiki_pages = jsonl_dir_to_df(\"data/wiki-pages\")\n",
        "mapping = generate_evidence_to_wiki_pages_mapping(\n",
        "    wiki_pages,\n",
        ")\n",
        "del wiki_pages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0j43olhFzS0u"
      },
      "outputs": [],
      "source": [
        "train_data = load_json(\"data/train_doc5sent5.jsonl\")\n",
        "dev_data = load_json(\"data/dev_doc5sent5.jsonl\")\n",
        "\n",
        "train_pkl_file = Path(\"data/train_doc5sent5.pkl\")\n",
        "if not train_pkl_file.exists():\n",
        "    train_df = join_with_topk_evidence(\n",
        "        pd.DataFrame(train_data),\n",
        "        mapping,\n",
        "        topk=EVIDENCE_TOPK,\n",
        "    )\n",
        "    train_df.to_pickle(train_pkl_file, protocol=4)\n",
        "else:\n",
        "    with open(train_pkl_file, \"rb\") as f:\n",
        "        train_df = pickle.load(f)\n",
        "\n",
        "dev_pkl_file = Path(\"data/dev_doc5sent5.pkl\")\n",
        "if not dev_pkl_file.exists():\n",
        "    dev_df = join_with_topk_evidence(\n",
        "        pd.DataFrame(dev_data),\n",
        "        mapping,\n",
        "        mode=\"eval\",\n",
        "        topk=EVIDENCE_TOPK,\n",
        "    )\n",
        "    dev_df.to_pickle(dev_pkl_file, protocol=4)\n",
        "else:\n",
        "    with open(dev_pkl_file, \"rb\") as f:\n",
        "        dev_df = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20ZDBS2ztdgX"
      },
      "outputs": [],
      "source": [
        "class AicupTopkEvidenceBERTDataset(BERTDataset):\n",
        "    \"\"\"AICUP dataset with top-k evidence sentences.\"\"\"\n",
        "\n",
        "    def __getitem__(\n",
        "        self,\n",
        "        idx: int,\n",
        "        **kwargs,\n",
        "    ) -> Tuple[Dict[str, torch.Tensor], int]:\n",
        "        item = self.data.iloc[idx]\n",
        "        claim = item[\"claim\"]\n",
        "        evidence = item[\"evidence_list\"]\n",
        "\n",
        "        # In case there are less than topk evidence sentences\n",
        "        pad = [\"[PAD]\"] * (self.topk - len(evidence))\n",
        "        evidence += pad\n",
        "        concat_claim_evidence = \" [SEP] \".join([*claim, *evidence])\n",
        "\n",
        "        concat = self.tokenizer(\n",
        "            concat_claim_evidence,\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.max_length,\n",
        "            truncation=True,\n",
        "        )\n",
        "        label = label2ID[item[\"label\"]] if \"label\" in item else -1\n",
        "        concat_ten = {k: torch.tensor(v) for k, v in concat.items()}\n",
        "\n",
        "        if \"label\" in item:\n",
        "            concat_ten[\"labels\"] = torch.tensor(label)\n",
        "        return concat_ten"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O0rVk3990DlD"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = \"bert-base-chinese\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Set up torch Datasets.\n",
        "train_dataset = AicupTopkEvidenceBERTDataset(\n",
        "    train_df,\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=MAX_SEQ_LEN,\n",
        ")\n",
        "val_dataset = AicupTopkEvidenceBERTDataset(\n",
        "    dev_df,\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=MAX_SEQ_LEN,\n",
        ")\n",
        "\n",
        "# Set up torch DataLoaders.\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    shuffle=True,\n",
        "    batch_size=TRAIN_BATCH_SIZE,\n",
        ")\n",
        "eval_dataloader = DataLoader(val_dataset, batch_size=TEST_BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CzMgs-Zs3sTN"
      },
      "outputs": [],
      "source": [
        "# Set up our model.\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    num_labels=len(label2ID),\n",
        ")\n",
        "model.to(device)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=LR)\n",
        "num_training_steps = NUM_EPOCHS * len(train_dataloader)\n",
        "lr_scheduler = set_lr_scheduler(optimizer, num_training_steps)\n",
        "\n",
        "# Set up the TensorBoard writer\n",
        "writer = SummaryWriter(log_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_eqDW9XMMkG"
      },
      "outputs": [],
      "source": [
        "def run_evaluation(model: torch.nn.Module, dataloader: DataLoader, device):\n",
        "    model.eval()\n",
        "\n",
        "    loss = 0\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader):\n",
        "            y_true.extend(batch[\"labels\"].tolist())\n",
        "\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            outputs = model(**batch)\n",
        "            loss += outputs.loss.item()\n",
        "            logits = outputs.logits\n",
        "            y_pred.extend(torch.argmax(logits, dim=1).tolist())\n",
        "\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "\n",
        "    return {\"val_loss\": loss / len(dataloader), \"val_acc\": acc}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_aqMjEek3wmu"
      },
      "outputs": [],
      "source": [
        "# Get ready to training.\n",
        "progress_bar = tqdm(range(num_training_steps))\n",
        "current_steps = 0\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    model.train()\n",
        "\n",
        "    for batch in train_dataloader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        progress_bar.update(1)\n",
        "        writer.add_scalar(\"training_loss\", loss.item(), current_steps)\n",
        "\n",
        "        y_pred = torch.argmax(outputs.logits, dim=1).tolist()\n",
        "\n",
        "        y_true = batch[\"labels\"].tolist()\n",
        "        # print(f\"batch train acc: {accuracy_score(y_true, y_pred)}\")\n",
        "\n",
        "        current_steps += 1\n",
        "\n",
        "        if current_steps % VALIDATION_STEP == 0 and current_steps > 0:\n",
        "            print(\"Start validation\")\n",
        "            val_results = run_evaluation(model, eval_dataloader, device)\n",
        "\n",
        "            # log each metric separately to TensorBoard\n",
        "            for metric_name, metric_value in val_results.items():\n",
        "                print(f\"{metric_name}: {metric_value}\")\n",
        "                writer.add_scalar(f\"{metric_name}\", metric_value, current_steps)\n",
        "\n",
        "            save_checkpoint(\n",
        "                model,\n",
        "                ckpt_dir,\n",
        "                current_steps,\n",
        "                mark=f\"val_acc={val_results['val_acc']:.4f}\",\n",
        "            )\n",
        "\n",
        "print(\"Finished training!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwbehxDP4v5-"
      },
      "outputs": [],
      "source": [
        "def run_predict(model: torch.nn.Module, test_dl: DataLoader, device) -> list:\n",
        "    model.eval()\n",
        "\n",
        "    preds = []\n",
        "    for batch in tqdm(test_dl, total=len(test_dl), leave=False, desc=\"Predicting\"):\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        pred = model(**batch).logits\n",
        "        pred = torch.argmax(pred, dim=1)\n",
        "        preds.extend(pred.tolist())\n",
        "    return preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zLkfuoAE49mz"
      },
      "outputs": [],
      "source": [
        "test_data = load_json(\"data/test_doc5sent5.jsonl\")\n",
        "test_pkl_file = Path(\"data/test_doc5sent5.pkl\")\n",
        "if not test_pkl_file.exists():\n",
        "    test_df = join_with_topk_evidence(\n",
        "        pd.DataFrame(test_data),\n",
        "        mapping,\n",
        "        mode=\"eval\",\n",
        "        topk=EVIDENCE_TOPK,\n",
        "    )\n",
        "    test_df.to_pickle(test_pkl_file, protocol=4)\n",
        "else:\n",
        "    with open(test_pkl_file, \"rb\") as f:\n",
        "        test_df = pickle.load(f)\n",
        "\n",
        "test_dataset = AicupTopkEvidenceBERTDataset(\n",
        "    test_df,\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=MAX_SEQ_LEN,\n",
        ")\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=TEST_BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqIjlht8yCMA"
      },
      "outputs": [],
      "source": [
        "# Start prediction\n",
        "\n",
        "ckpt_name = \"val_acc=0.4208_model.75.pt\"    # You need to change your best checkpoint.\n",
        "model = load_model(model, ckpt_name, ckpt_dir)\n",
        "predicted_label = run_predict(model, test_dataloader, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gl9I3ZWW4pHo"
      },
      "outputs": [],
      "source": [
        "# Write prediction file\n",
        "\n",
        "predict_dataset = test_df.copy()\n",
        "predict_dataset[\"predicted_label\"] = list(map(ID2label.get, predicted_label))\n",
        "predict_dataset[[\"id\", \"predicted_label\", \"predicted_evidence\"]].to_json(\n",
        "    OUTPUT_FILENAME,\n",
        "    orient=\"records\",\n",
        "    lines=True,\n",
        "    force_ascii=False,\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "10286f3c74912972f7d1fdceceee5be5b7c77248e5efe5afcbc6a71f24d230fa"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
